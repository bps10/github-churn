{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model\n",
    "\n",
    "[Pyspark blog](https://bryancutler.github.io/)\n",
    "\n",
    "[ML w/ Pyspark](https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa)\n",
    "\n",
    "[GitHub event types](https://developer.github.com/v3/activity/events/types/)\n",
    "\n",
    "[Churn modeling](https://www.urbanairship.com/blog/churn-prediction-our-machine-learning-model)\n",
    "\n",
    "[Interpreting Trees](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27)\n",
    "\n",
    "[Intro to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html#why-introduce-the-general-principle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, Binarizer#, OneHotEncoderEstimator, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, LogisticRegressionModel, GBTClassifier, RandomForestClassifier\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, datediff, unix_timestamp\n",
    "\n",
    "import helper as h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model name and filter flags for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_filter = 0\n",
    "high_low_filter = 1\n",
    "if company_filter:\n",
    "    model_name = 'company_' + str(company_filter)\n",
    "else:\n",
    "    model_name = 'company_' + str(company_filter) + 'high_low_' + str(high_low_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = h.get_merged_data('classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_data = _data.withColumn(\"end_date\", to_timestamp('2016-06-02 23:59:59+00:00'))\n",
    "#_data = _data.withColumn(\"T\", datediff(_data.end_data, _data.created_at))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load K-Means classifier\n",
    "\n",
    "Classify users as high or low use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = h.add_high_low_flag(_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = h.feature_scaling(_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and segment users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data = _data.filter((_data.company == company_filter)) \n",
    "if not company_filter:\n",
    "    churn_data = churn_data[churn_data.high_low_user == high_low_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.print_user_churn(churn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [t[0] for t in churn_data.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "numeric_features.remove('second_period_event_count')\n",
    "numeric_features.remove('frequency')\n",
    "numeric_features.remove('non_passive_events')\n",
    "numeric_features.remove('public_repos_gists')\n",
    "numeric_features.remove('high_low_user')\n",
    "numeric_features.remove('company')\n",
    "numeric_features.remove('time_between_first_last_event')\n",
    "numeric_features.remove('recency')\n",
    "#churn_data.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build PySpark pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "# binarizer needs double type or it throws an error.\n",
    "churn_data = churn_data.withColumn(\"second_period_event_count\", \n",
    "                                   churn_data.second_period_event_count.cast(DoubleType()))\n",
    "binarizer = Binarizer(threshold=0.5, \n",
    "                      inputCol=\"second_period_event_count\", \n",
    "                      outputCol=\"label\")\n",
    "\n",
    "stages += [binarizer]\n",
    "assembler = VectorAssembler(inputCols=numeric_features, \n",
    "                            outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(churn_data)\n",
    "churn_data = pipelineModel.transform(churn_data)\n",
    "selectedCols = ['label', 'features'] + numeric_features\n",
    "churn_data = churn_data.select(selectedCols)\n",
    "churn_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pipeline\n",
    "pipeline.write().overwrite().save('pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examples = pd.DataFrame(churn_data.take(5), columns=churn_data.columns).transpose()\n",
    "#examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = churn_data.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: {0}\".format(train.count()))\n",
    "print(\"Test Dataset Count: {0}\".format(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=50,\n",
    "                        #regParam=0.3, elasticNetParam=0.08\n",
    "                       )\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.sort(lrModel.coefficients)\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': numeric_features, # np.asarray(numeric_features)[lrModel.coefficients.indices], \n",
    "              'weights': lrModel.coefficients.values}\n",
    "            ).sort_values(by='weights', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "fig.set_tight_layout(True)\n",
    "pd.DataFrame({'features': numeric_features, # np.asarray(numeric_features)[lrModel.coefficients.indices], \n",
    "              'weights': lrModel.coefficients.values}\n",
    "            ).sort_values(by='weights', ascending=False)\n",
    "ax.plot(roc['FPR'],roc['TPR'])\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "#ax.title('ROC Curve')\n",
    "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "\n",
    "fig.savefig('figures/logistic_regression_ROC.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = trainingSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lrModel._java_obj.setThreshold(0.75)\n",
    "print('Threshold: {0}'.format(lrModel._java_obj.getThreshold()))\n",
    "predictions = lrModel.transform(test)\n",
    "show_cols = ['followers_count', 'blog', \n",
    "             'label', 'rawPrediction', 'prediction', 'probability']\n",
    "predictions.select(show_cols).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.eval_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel.write().overwrite().save('lrModel_' + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "[Gradient Boost vs XGboost](https://datascience.stackexchange.com/questions/16904/gbm-vs-xgboost-key-differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to fullfile\n",
      "+-------------------+-------+----+-----+--------------------+----------+--------------------+\n",
      "|    followers_count|company|blog|label|       rawPrediction|prediction|         probability|\n",
      "+-------------------+-------+----+-----+--------------------+----------+--------------------+\n",
      "|0.47712125471966244|      0|   1|  0.0|[0.27226970042930...|       0.0|[0.63286776479089...|\n",
      "| 1.0413926851582251|      0|   0|  0.0|[-0.1774756260228...|       1.0|[0.41218227251353...|\n",
      "| 0.6020599913279624|      0|   0|  0.0|[0.28653914035891...|       0.0|[0.63947316852387...|\n",
      "| 0.3010299956639812|      0|   0|  0.0|[0.44483160176320...|       0.0|[0.70882066470747...|\n",
      "|0.47712125471966244|      0|   0|  0.0|[-0.0731294011844...|       1.0|[0.46350034184256...|\n",
      "|0.47712125471966244|      0|   0|  0.0|[0.83078297981233...|       0.0|[0.84044810251930...|\n",
      "|                0.0|      0|   1|  0.0|[-0.3728070131871...|       1.0|[0.32177773262117...|\n",
      "|                0.0|      0|   0|  0.0|[0.15932174979721...|       0.0|[0.57899362707934...|\n",
      "|                0.0|      0|   0|  0.0|[0.68962978098022...|       0.0|[0.79887205632900...|\n",
      "|                0.0|      0|   0|  0.0|[0.18111533320081...|       0.0|[0.58958030803880...|\n",
      "+-------------------+-------+----+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Precision: 0.687\n",
      "Recall:    0.462\n",
      "Accuracy:  0.783\n",
      "F1-score:  0.5525\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "h.write_tree_to_file(gbtModel.toDebugString, 'gbt_trees_' + model_name)\n",
    "\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select(show_cols).show(10)\n",
    "\n",
    "h.eval_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "[pyspark feature imortances doc](http://spark.apache.org/docs/2.1.1/api/python/pyspark.ml.html#pyspark.ml.classification.GBTClassificationModel.featureImportances):\n",
    "\n",
    "Estimate of the importance of each feature.\n",
    "\n",
    "Each feature’s importance is the average of its importance across all trees in the ensemble. The importance vector is normalized to sum to 1. This method is suggested by Hastie et al. (Hastie, Tibshirani, Friedman. “The Elements of Statistical Learning, 2nd Edition.” 2001.) and follows the implementation from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>public_repos_count</td>\n",
       "      <td>0.221444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time_between_first_last_event</td>\n",
       "      <td>0.170778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CreateEvent_count</td>\n",
       "      <td>0.131313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>following_count</td>\n",
       "      <td>0.0890643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IssuesEvent_count</td>\n",
       "      <td>0.0853046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ForkEvent_count</td>\n",
       "      <td>0.0680775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PushEvent_count</td>\n",
       "      <td>0.056275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WatchEvent_count</td>\n",
       "      <td>0.0502538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IssueCommentEvent_count</td>\n",
       "      <td>0.0398236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>followers_count</td>\n",
       "      <td>0.0360821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>public_gists_count</td>\n",
       "      <td>0.0174675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blog</td>\n",
       "      <td>0.00950378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DeleteEvent_count</td>\n",
       "      <td>0.00819153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MemberEvent_count</td>\n",
       "      <td>0.00727451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PullRequestEvent_count</td>\n",
       "      <td>0.00336266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hireable</td>\n",
       "      <td>0.0021269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PullRequestReviewCommentEvent_count</td>\n",
       "      <td>0.0014599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CommitCommentEvent_count</td>\n",
       "      <td>0.00111251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ReleaseEvent_count</td>\n",
       "      <td>0.000602691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PublicEvent_count</td>\n",
       "      <td>0.000482981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0            1\n",
       "3                    public_repos_count     0.221444\n",
       "6         time_between_first_last_event     0.170778\n",
       "8                     CreateEvent_count     0.131313\n",
       "1                       following_count    0.0890643\n",
       "12                    IssuesEvent_count    0.0853046\n",
       "10                      ForkEvent_count    0.0680775\n",
       "17                      PushEvent_count     0.056275\n",
       "19                     WatchEvent_count    0.0502538\n",
       "11              IssueCommentEvent_count    0.0398236\n",
       "0                       followers_count    0.0360821\n",
       "4                    public_gists_count    0.0174675\n",
       "2                                  blog   0.00950378\n",
       "9                     DeleteEvent_count   0.00819153\n",
       "13                    MemberEvent_count   0.00727451\n",
       "15               PullRequestEvent_count   0.00336266\n",
       "5                              hireable    0.0021269\n",
       "16  PullRequestReviewCommentEvent_count    0.0014599\n",
       "7              CommitCommentEvent_count   0.00111251\n",
       "18                   ReleaseEvent_count  0.000602691\n",
       "14                    PublicEvent_count  0.000482981"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gbtModel.featureImportances.indices)\n",
    "numeric_features = np.asarray(numeric_features)\n",
    "pd.DataFrame([numeric_features[gbtModel.featureImportances.indices], \n",
    "              gbtModel.featureImportances.values]).T.sort_values(by=[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evaluator = BinaryClassificationEvaluator()\\nprint(\"Test Area Under ROC: \" + str(\\n    evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(\n",
    "    evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\\nrfModel = rf.fit(train)\\nh.write_tree_to_file(rfModel.toDebugString, 'rf_trees_' + model_name)\\npredictions = rfModel.transform(test)\\npredictions.select(show_cols).show(10)\\nh.eval_metrics(predictions)\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "h.write_tree_to_file(rfModel.toDebugString, 'rf_trees_' + model_name)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select(show_cols).show(10)\n",
    "h.eval_metrics(predictions)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o908.cache.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat scala.collection.immutable.HashSet$HashTrieSet.updated0(HashSet.scala:557)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:84)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:35)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:22)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.SetBuilder.$plus$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.TraversableLike$class.to(TraversableLike.scala:590)\n\tat scala.collection.AbstractTraversable.to(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:304)\n\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild$lzycompute(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$1(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1$$anonfun$apply$6.apply(TreeNode.scala:224)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1.apply(TreeNode.scala:224)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:217)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:193)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:191)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet.add(ExpressionSet.scala:63)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet$$anonfun$$plus$plus$1.apply(ExpressionSet.scala:79)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-dea7b637ea98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Run cross validations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mvalidateUB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandCol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mvalidateLB\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandCol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvalidateUB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \"\"\"\n\u001b[1;32m    604\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o908.cache.\n: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat scala.collection.immutable.HashSet$HashTrieSet.updated0(HashSet.scala:557)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:84)\n\tat scala.collection.immutable.HashSet.$plus(HashSet.scala:35)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:22)\n\tat scala.collection.mutable.SetBuilder.$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.generic.Growable$$anonfun$$plus$plus$eq$1.apply(Growable.scala:59)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.SetBuilder.$plus$plus$eq(SetBuilder.scala:20)\n\tat scala.collection.TraversableLike$class.to(TraversableLike.scala:590)\n\tat scala.collection.AbstractTraversable.to(Traversable.scala:104)\n\tat scala.collection.TraversableOnce$class.toSet(TraversableOnce.scala:304)\n\tat scala.collection.AbstractTraversable.toSet(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild$lzycompute(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.containsChild(TreeNode.scala:88)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$1(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1$$anonfun$apply$6.apply(TreeNode.scala:224)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$1.apply(TreeNode.scala:224)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:217)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:193)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:191)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet.add(ExpressionSet.scala:63)\n\tat org.apache.spark.sql.catalyst.expressions.ExpressionSet$$anonfun$$plus$plus$1.apply(ExpressionSet.scala:79)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lrModel.elasticNetParam, [0, 0.8, 0.08])\n",
    "             .addGrid(lrModel.regParam, [0, 0.3, 0.003])\n",
    "             .addGrid(lrModel.maxIter, [50, 100])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=lrModel, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'features': numeric_features, # np.asarray(numeric_features)[lrModel.coefficients.indices], \n",
    "              'weights': lrModel.coefficients.values}\n",
    "            ).sort_values(by='weights', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.eval_metrics(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
